For Lab 3, I have done snorkel lab 1, these are the changes I made to the file:

1. Added 1 new SPAM LF, which classifies a comment as SPAM if a link(URL) is detected
2. Added 1 new Paws LF, which classfies a comment as Paws if words like "love" or "amazing" are detected.
3. Used new classifiers instead of the Logistic Regression.
%matplotlib inline

import os

# Turn off TensorFlow logging messages
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

# For reproducibility
os.environ["PYTHONHASHSEED"] = "0"
%matplotlib inline

import os

# Turn off TensorFlow logging messages
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

# For reproducibility
os.environ["PYTHONHASHSEED"] = "0"
import pandas as pd

DISPLAY_ALL_TEXT = False

pd.set_option("display.max_colwidth", 0 if DISPLAY_ALL_TEXT else 50)
pip install snorkel
pip install spacy
# Download the spaCy english model
!python -m spacy download en_core_web_sm
from utils import load_spam_dataset

df_train, df_test = load_spam_dataset()

# We pull out the label vectors for ease of use later
Y_test = df_test.label.values
# For clarity, we define constants to represent the class labels for spam, Paws, and abstaining.
ABSTAIN = -1
Paws = 0
SPAM = 1
Writing Label Function (LFs)
df_train.head()
df_train[["author", "text", "video"]].sample(20, random_state=2)
Writing a LF that filters out all "Check Out Comments"
from snorkel.labeling import labeling_function


@labeling_function()
def check(x):
    return SPAM if "check" in x.text.lower() else ABSTAIN


@labeling_function()
def check_out(x):
    return SPAM if "check out" in x.text.lower() else ABSTAIN
MY OWN SPAM DETECTOR FUNCTION:

If the function detects a link of any sort, it classifies it as SPAM
@labeling_function()
def contains_link(x):
    """Label as SPAM if the comment contains a URL or shortened link."""
    if "http" in x.text.lower() or "www." in x.text.lower() or "bit.ly" in x.text.lower():
        return SPAM
    else:
        return ABSTAIN
MY OWN Paws
 DETECTOR FUNCTION:

If the function detects a link of any sort, it classifies it as SPAM
from snorkel.labeling import labeling_function

@labeling_function()
def positive_comment(x):
    """Label as Paws if the comment expresses general praise or positivity."""
    positive_words = ["love", "amazing", "awesome", "great", "nice", "beautiful", "cool"]
    text = x.text.lower()
    return Paws if any(word in text for word in positive_words) else ABSTAIN

LF Applier
from snorkel.labeling import PandasLFApplier

lfs = [check_out, check, contains_link, positive_comment]

applier = PandasLFApplier(lfs=lfs)
L_train = applier.apply(df=df_train)

L_train
Evaluating performance on Training Set
coverage_check_out, coverage_check = (L_train != ABSTAIN).mean(axis=0)
print(f"check_out coverage: {coverage_check_out * 100:.6f}%")
print(f"check coverage: {coverage_check * 100:.6f}%")
coverage = (L_train != ABSTAIN).mean(axis=0)

for i, cov in enumerate(coverage):
    print(f"LF {i} coverage: {cov * 100:.6f}%")

from snorkel.labeling import LFAnalysis

LFAnalysis(L=L_train, lfs=lfs).lf_summary()
df_train.iloc[L_train[:, 1] == SPAM].sample(10, random_state=1)
from snorkel.analysis import get_label_buckets

buckets = get_label_buckets(L_train[:, 0], L_train[:, 1])
df_train.iloc[buckets[(ABSTAIN, SPAM)]].sample(10, random_state=1)
Balance Accuracy and Coverage
import re


@labeling_function()
def regex_check_out(x):
    return SPAM if re.search(r"check.*out", x.text, flags=re.I) else ABSTAIN
lfs = [check_out, check, regex_check_out]

applier = PandasLFApplier(lfs=lfs)
L_train = applier.apply(df=df_train)
LFAnalysis(L=L_train, lfs=lfs).lf_summary()

buckets = get_label_buckets(L_train[:, 1], L_train[:, 2])
df_train.iloc[buckets[(SPAM, ABSTAIN)]].sample(10, random_state=1)
Writing LF that uses thired party model
from snorkel.preprocess import preprocessor
from textblob import TextBlob


@preprocessor(memoize=True)
def textblob_sentiment(x):
    scores = TextBlob(x.text)
    x.polarity = scores.sentiment.polarity
    x.subjectivity = scores.sentiment.subjectivity
    return x
@labeling_function(pre=[textblob_sentiment])
def textblob_polarity(x):
    return Paws if x.polarity > 0.9 else ABSTAIN
@labeling_function(pre=[textblob_sentiment])
def textblob_subjectivity(x):
    return Paws if x.subjectivity >= 0.5 else ABSTAIN
lfs = [textblob_polarity, textblob_subjectivity]

applier = PandasLFApplier(lfs)
L_train = applier.apply(df_train)
LFAnalysis(L_train, lfs).lf_summary()
More Labeling Functions
from snorkel.labeling import LabelingFunction


def keyword_lookup(x, keywords, label):
    if any(word in x.text.lower() for word in keywords):
        return label
    return ABSTAIN


def make_keyword_lf(keywords, label=SPAM):
    return LabelingFunction(
        name=f"keyword_{keywords[0]}",
        f=keyword_lookup,
        resources=dict(keywords=keywords, label=label),
    )


"""Spam comments talk about 'my channel', 'my video', etc."""
keyword_my = make_keyword_lf(keywords=["my"])

"""Spam comments ask users to subscribe to their channels."""
keyword_subscribe = make_keyword_lf(keywords=["subscribe"])

"""Spam comments post links to other channels."""
keyword_link = make_keyword_lf(keywords=["http"])

"""Spam comments make requests rather than commenting."""
keyword_please = make_keyword_lf(keywords=["please", "plz"])

"""Paws comments actually talk about the video's content."""
keyword_song = make_keyword_lf(keywords=["song"], label=Paws)
@labeling_function()
def short_comment(x):
    """Paws comments are often short, such as 'cool video!'"""
    return Paws if len(x.text.split()) < 5 else ABSTAIN
from snorkel.preprocess.nlp import SpacyPreprocessor

# The SpacyPreprocessor parses the text in text_field and
# stores the new enriched representation in doc_field
spacy = SpacyPreprocessor(text_field="text", doc_field="doc", memoize=True)
@labeling_function(pre=[spacy])
def has_person(x):
    """Paws comments mention specific people and are short."""
    if len(x.doc) < 20 and any([ent.label_ == "PERSON" for ent in x.doc.ents]):
        return Paws
    else:
        return ABSTAIN
from snorkel.labeling.lf.nlp import nlp_labeling_function


@nlp_labeling_function()
def has_person_nlp(x):
    """Paws comments mention specific people and are short."""
    if len(x.doc) < 20 and any([ent.label_ == "PERSON" for ent in x.doc.ents]):
        return Paws
    else:
        return ABSTAIN
Combining  label function outputs with the label model
lfs = [
    keyword_my,
    keyword_subscribe,
    keyword_link,
    keyword_please,
    keyword_song,
    regex_check_out,
    short_comment,
    has_person_nlp,
    textblob_polarity,
    textblob_subjectivity,
]
applier = PandasLFApplier(lfs=lfs)
L_train = applier.apply(df=df_train)
L_test = applier.apply(df=df_test)
LFAnalysis(L=L_train, lfs=lfs).lf_summary()
import matplotlib.pyplot as plt

%matplotlib inline


def plot_label_frequency(L):
    plt.hist((L != ABSTAIN).sum(axis=1), density=True, bins=range(L.shape[1]))
    plt.xlabel("Number of labels")
    plt.ylabel("Fraction of dataset")
    plt.show()


plot_label_frequency(L_train)
from snorkel.labeling.model import MajorityLabelVoter

majority_model = MajorityLabelVoter()
preds_train = majority_model.predict(L=L_train)
preds_train
from snorkel.labeling.model import LabelModel

label_model = LabelModel(cardinality=2, verbose=True)
label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)
majority_acc = majority_model.score(L=L_test, Y=Y_test, tie_break_policy="random")[
    "accuracy"
]
print(f"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%")

label_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy="random")[
    "accuracy"
]
print(f"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%")
def plot_probabilities_histogram(Y):
    plt.hist(Y, bins=10)
    plt.xlabel("Probability of SPAM")
    plt.ylabel("Number of data points")
    plt.show()


probs_train = label_model.predict_proba(L=L_train)
plot_probabilities_histogram(probs_train[:, SPAM])
Filtering out Unlabeled Points
from snorkel.labeling import filter_unlabeled_dataframe

df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(
    X=df_train, y=probs_train, L=L_train
)
Training a Classifier
Converting data to numerical features
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(ngram_range=(1, 5))
X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())
X_test = vectorizer.transform(df_test.text.tolist())
from snorkel.utils import probs_to_preds

preds_train_filtered = probs_to_preds(probs=probs_train_filtered)
probs_train_filtered
SVM Model
from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV

# LinearSVC is faster than SVC with linear kernel
base_model = LinearSVC(C=1e3, max_iter=10000)
base_model.fit(X=X_train, y=preds_train_filtered)
print(f"Test Accuracy: {base_model.score(X=X_test, y=Y_test) * 100:.1f}%")
# Wrap it to get probability estimates
CaliClassifier = CalibratedClassifierCV(base_model, cv=3)
CaliClassifier.fit(X=X_train, y=preds_train_filtered)
print(f"Test Accuracy: {CaliClassifier.score(X=X_test, y=Y_test) * 100:.1f}%")
